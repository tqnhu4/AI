

## üöÄ **TensorFlow 2.x Cheat Sheet**

TensorFlow is an open-source library for machine learning and deep learning, developed by Google. It stands out for its tensor-based computations, flexible model building, and easy deployment.

-----

### **1. üß† Core Concepts**

  * **Tensor:** A multi-dimensional (n-dimensional) data array. This is the basic unit of data in TensorFlow.
      * **Scalar:** 0-dimensional tensor (a single number). `tf.constant(5)`
      * **Vector:** 1-dimensional tensor (a list of numbers). `tf.constant([1, 2, 3])`
      * **Matrix:** 2-dimensional tensor (a table of numbers). `tf.constant([[1, 2], [3, 4]])`
  * **Graph:** In TF 1.x, all operations were defined in a static graph. In TF 2.x, with **Eager Execution** by default, operations are executed immediately, just like regular Python, making debugging easier. Graphs still exist implicitly when you use `@tf.function` for performance optimization.
  * **Keras API:** A high-level API tightly integrated into TensorFlow 2.x, making model building and training significantly simpler and more intuitive.
  * **Layer:** The basic building blocks of a neural network (e.g., `Dense`, `Conv2D`, `MaxPooling2D`).
  * **Model:** A collection of interconnected layers that perform a specific task (e.g., image classification, machine translation).

-----

### **2. üî¢ Tensor Operations**

```python
import tensorflow as tf
import numpy as np

# Create Tensors
scalar = tf.constant(7)
vector = tf.constant([1, 2, 3], dtype=tf.float32)
matrix = tf.constant([[1, 2], [3, 4]])
tensor_3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])

# Check Tensor properties
print(f"Scalar: {scalar.shape}, Dtype: {scalar.dtype}") # () (scalar), int32
print(f"Vector: {vector.shape}, Dtype: {vector.dtype}") # (3,) (vector), float32
print(f"Matrix: {matrix.shape}, Dtype: {matrix.dtype}") # (2, 2) (matrix), int32
print(f"Tensor 3D: {tensor_3d.shape}") # (2, 2, 2)

# Convert between NumPy and Tensor
numpy_array = np.array([5, 6, 7])
tensor_from_np = tf.convert_to_tensor(numpy_array)
tensor_to_np = tensor_3d.numpy()

# Basic Operations
a = tf.constant([1, 2])
b = tf.constant([3, 4])
print(f"Addition: {a + b}")         # tf.add(a, b)
print(f"Multiplication: {a * b}")         # tf.multiply(a, b)
print(f"Matrix Multiplication: {tf.matmul(tf.constant([[1,2]]), tf.constant([[3],[4]]))}")

# Reshaping
reshaped_tensor = tf.reshape(tensor_3d, (8,))
print(f"Reshaped: {reshaped_tensor}")

# Expanding Dimensions
expanded_tensor = tf.expand_dims(vector, axis=0) # Add a dimension at the beginning
print(f"Expanded: {expanded_tensor.shape}") # (1, 3)

# Squeeze (Remove dimensions of size 1)
squeezed_tensor = tf.squeeze(expanded_tensor)
print(f"Squeezed: {squeezed_tensor.shape}") # (3,)
```

-----

### **3. üèóÔ∏è Building Keras Models**

Keras is the primary way to build models in TensorFlow 2.x.

#### **3.1. Sequential Model**

Suitable for simple, stacked layer architectures.

```python
from tensorflow.keras import layers, models

model_seq = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(784,)), # Input and first hidden layer
    layers.Dropout(0.5), # Regularization
    layers.Dense(10, activation='softmax') # Output layer for 10-class classification
])
model_seq.summary() # Summarize model structure
```

#### **3.2. Functional API Model**

Allows building more complex models with multiple inputs/outputs or branches.

```python
from tensorflow.keras import Input

input_layer = Input(shape=(784,))
x = layers.Dense(64, activation='relu')(input_layer)
output_layer = layers.Dense(10, activation='softmax')(x)
model_func = models.Model(inputs=input_layer, outputs=output_layer)
model_func.summary()
```

#### **3.3. Model Subclassing**

The most customizable way, where you define your own `Model` class.

```python
class MyModel(models.Model):
    def __init__(self, num_classes=10):
        super().__init__()
        self.dense1 = layers.Dense(64, activation='relu')
        self.dropout = layers.Dropout(0.5)
        self.dense2 = layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.dense1(inputs)
        x = self.dropout(x)
        return self.dense2(x)

model_sub = MyModel(num_classes=10)
# model_sub.build(input_shape=(None, 784)) # Needs to be built or called with actual input
# model_sub.summary()
```

-----

### **4. üõ†Ô∏è Common Layers**

  * **`layers.Dense(units, activation='relu')`:** Fully connected layer.
  * **`layers.Conv2D(filters, kernel_size, activation='relu', input_shape)`:** 2D Convolutional layer (for images).
  * **`layers.MaxPooling2D(pool_size)`:** 2D Max pooling layer (reduces image data dimensions).
  * **`layers.Flatten()`:** Flattens a tensor into a 1D vector.
  * **`layers.Dropout(rate)`:** Randomly drops out a fraction of neurons to prevent overfitting.
  * **`layers.LSTM(units)` / `layers.GRU(units)`:** Recurrent Neural Network layers for sequence data.
  * **`layers.Embedding(input_dim, output_dim)`:** Represents words as vectors (for NLP).

-----

### **5. ‚öôÔ∏è Compile & Fit**

```python
# 1. Compile the model (Configure the learning process)
model.compile(
    optimizer='adam',                            # Optimization algorithm (e.g., 'adam', 'sgd', 'rmsprop')
    loss='sparse_categorical_crossentropy',      # Loss function (e.g., 'mse', 'binary_crossentropy')
                                                 # 'sparse_categorical_crossentropy' if labels are integers
                                                 # 'categorical_crossentropy' if labels are one-hot encoded
    metrics=['accuracy']                         # Evaluation metrics (e.g., 'accuracy', 'mae', 'recall')
)

# 2. Train the model
# Assuming x_train, y_train, x_test, y_test are loaded and preprocessed
# x_train: training data, y_train: training labels
# epochs: number of times the entire dataset is iterated over
# batch_size: number of samples per gradient update
# validation_data: data to evaluate model performance during training
model.fit(
    x_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(x_test, y_test)
)

# 3. Evaluate the model
loss, accuracy = model.evaluate(x_test, y_test, verbose=0)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")

# 4. Predict
predictions = model.predict(x_new_data)
predicted_classes = tf.argmax(predictions, axis=1) # If output is softmax
```

-----

### **6. üíæ Save & Load Models**

```python
# Save the entire model (architecture, weights, training configuration)
model.save('my_model.h5') # HDF5 format (recommended for entire model)
# Or SavedModel format (default and more flexible for deployment)
model.save('my_saved_model')

# Load the model
loaded_model_h5 = tf.keras.models.load_model('my_model.h5')
loaded_model_saved = tf.keras.models.load_model('my_saved_model')

# Save/Load only weights
model.save_weights('my_model_weights.h5')
model.load_weights('my_model_weights.h5')
```

-----

### **7. üîÑ Custom Training (Callbacks & Custom Training Loop)**

#### **7.1. Callbacks**

Functions called at various stages during training.

  * `tf.keras.callbacks.EarlyStopping`: Stops training early if the model isn't improving.
  * `tf.keras.callbacks.ModelCheckpoint`: Saves the best model during training.
  * `tf.keras.callbacks.TensorBoard`: Logs training data for visualization with TensorBoard.

<!-- end list -->

```python
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stopping = EarlyStopping(monitor='val_loss', patience=3)
checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True)

# model.fit(x_train, y_train, epochs=100, callbacks=[early_stopping, checkpoint])
```

#### **7.2. Custom Training Loop**

For very specific cases requiring full control over the training process.

```python
# Simple example of a custom training loop
# @tf.function # Optional: to compile into a static graph for optimization
# def train_step(images, labels):
#     with tf.GradientTape() as tape:
#         predictions = model(images)
#         loss = loss_object(labels, predictions)
#     gradients = tape.gradient(loss, model.trainable_variables)
#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))
#     train_loss_metric.update_state(loss)
#     train_accuracy_metric.update_state(labels, predictions)

# for epoch in range(EPOCHS):
#     for images, labels in train_dataset:
#         train_step(images, labels)
#     # ... (logic for validation and logging)
```

-----

### **8. üñ•Ô∏è Visualization with TensorBoard**

To launch TensorBoard and view training logs:

1.  In your Python code, add the `TensorBoard` callback when calling `model.fit()`:
    ```python
    import datetime
    log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

    # model.fit(x_train, y_train, epochs=10, callbacks=[tensorboard_callback])
    ```
2.  After training, open your terminal or Anaconda Prompt and navigate to your project's root directory (where the `logs` folder was created).
3.  Run the following command:
    ```bash
    tensorboard --logdir logs/fit
    ```
4.  Open your web browser and go to the displayed address (usually `http://localhost:6006/`).

-----

This cheat sheet will give you a quick overview of how to work with TensorFlow 2.x. Remember, the best way to learn is by doing\!