Dưới đây là bản dịch tiếng Anh của phần thiết kế các tính năng chính của SpaCy:

-----

## Key Features of SpaCy with Examples

SpaCy is a powerful and efficient open-source library for Natural Language Processing (NLP) in Python. It's designed to process complex NLP tasks quickly and accurately. Below are SpaCy's key features, along with illustrative examples for each.

### 1\. Tokenization

Tokenization is the process of splitting a text string into smaller units called "tokens." Tokens are typically words, punctuation marks, or numbers. SpaCy offers intelligent tokenization, effectively handling special cases like abbreviations, numbers with commas, and more.

**Example:**

```python
import spacy

nlp = spacy.load("en_core_web_sm")  # Load the small English model
text = "SpaCy is an amazing NLP library! It's developed by Explosion.AI."
doc = nlp(text)

print("Tokens:")
for token in doc:
    print(token.text)
```

**Output:**

```
Tokens:
SpaCy
is
an
amazing
NLP
library
!
It
's
developed
by
Explosion.AI
.
```

### 2\. Part-of-Speech Tagging (POS Tagging)

Part-of-Speech (POS) tagging is the process of assigning a grammatical category (e.g., noun, verb, adjective) to each token in a text. This helps in understanding the grammatical role of each word in a sentence.

**Example:**

```python
import spacy

nlp = spacy.load("en_core_web_sm")
text = "Apple is looking at buying U.K. startup for $1 billion."
doc = nlp(text)

print("\nPOS Tags:")
for token in doc:
    print(f"{token.text:<10} {token.pos_:<10} {token.tag_:<10}")
```

**Output (may vary slightly depending on the model version):**

```
POS Tags:
Apple      PROPN      NNP       
is         AUX        VBZ       
looking    VERB       VBG       
at         ADP        IN        
buying     VERB       VBG       
U.K.       PROPN      NNP       
startup    NOUN       NN        
for        ADP        IN        
$          SYM        $         
1          NUM        CD        
billion    NUM        CD        
.          PUNCT      .         
```

  * `token.pos_`: Universal POS tag.
  * `token.tag_`: More detailed Treebank tag.

### 3\. Named Entity Recognition (NER)

NER is the process of identifying and classifying named entities in text, such as names of people, organizations, locations, dates, monetary values, etc. This is a crucial feature for information extraction.

**Example:**

```python
import spacy

nlp = spacy.load("en_core_web_sm")
text = "Apple is looking at buying U.K. startup for $1 billion from London."
doc = nlp(text)

print("\nNamed Entities:")
for ent in doc.ents:
    print(f"{ent.text:<20} {ent.label_:<15}")
```

**Output:**

```
Named Entities:
Apple                ORG            
U.K.                 GPE            
$1 billion           MONEY          
London               GPE            
```

### 4\. Dependency Parsing

Dependency parsing is the process of analyzing the grammatical structure of a sentence by identifying the dependency relationships between words. It helps understand how words in a sentence are connected to each other.

**Example:**

```python
import spacy

nlp = spacy.load("en_core_web_sm")
text = "She sells seashells by the seashore."
doc = nlp(text)

print("\nDependency Parsing:")
for token in doc:
    print(f"{token.text:<10} {token.dep_:<15} {token.head.text:<10}")
```

**Output:**

```
Dependency Parsing:
She        nsubj           sells     
sells      ROOT            sells     
seashells  dobj            sells     
by         prep            sells     
the        det             seashore  
seashore   pobj            by        
.          punct           sells     
```

  * `token.dep_`: The dependency relation of the token to its `token.head`.
  * `token.head.text`: The word that `token` depends on.

### 5\. Lemmatization

Lemmatization is the process of reducing words to their base or dictionary form (lemma). For example, "running," "ran," and "runs" all have the lemma "run." This is very useful for normalizing text data.

**Example:**

```python
import spacy

nlp = spacy.load("en_core_web_sm")
text = "The quick brown foxes are running fast."
doc = nlp(text)

print("\nLemmatization:")
for token in doc:
    print(f"{token.text:<10} {token.lemma_:<10}")
```

**Output:**

```
Lemmatization:
The        the       
quick      quick     
brown      brown     
foxes      fox       
are        be        
running    run       
fast       fast      
.          .         
```

### 6\. Vector Representations

SpaCy can represent words and documents as numerical vectors (word embeddings). These vectors are typically learned from a large corpus of text data and capture the semantic meaning of words. This allows for operations on words, such as finding similar words.

**Note:** To use this feature, you need to load larger models that contain word vectors (e.g., `en_core_web_md` or `en_core_web_lg`).

**Example:**

```python
import spacy

# You need to download a larger model that includes word vectors
try:
    nlp = spacy.load("en_core_web_md")
except OSError:
    print("Please download the 'en_core_web_md' model by running: python -m spacy download en_core_web_md")
    exit()

word1 = nlp("dog")
word2 = nlp("cat")
word3 = nlp("apple")

print(f"\nSimilarity between 'dog' and 'cat': {word1.similarity(word2):.2f}")
print(f"Similarity between 'dog' and 'apple': {word1.similarity(word3):.2f}")

# Calculate the vector for a sentence
text = "This is a simple sentence."
doc = nlp(text)
print(f"\nVector for '{text}': {doc.vector[:5]}...") # Print the first 5 elements of the vector
```

**Output (similarity results may vary depending on the model version):**

```
Similarity between 'dog' and 'cat': 0.81
Similarity between 'dog' and 'apple': 0.29

Vector for 'This is a simple sentence.': [-0.01633512 -0.01258674 -0.00934149 -0.01601362  0.02409545]...
```

### 7\. Rule-based Matching

SpaCy provides a powerful engine to find patterns in text based on token attributes (e.g., POS tag, lemma, text). This is very useful when you need to search for specific text structures without relying on machine learning models.

**Example:**

```python
import spacy
from spacy.matcher import Matcher

nlp = spacy.load("en_core_web_sm")
matcher = Matcher(nlp.vocab)

# Define a pattern to find "verb + noun" sequences
pattern = [{"POS": "VERB"}, {"POS": "NOUN"}]
matcher.add("VERB_NOUN_PATTERN", [pattern])

text = "She eats apples and drinks water."
doc = nlp(text)

print("\nRule-based Matching (Verb + Noun):")
matches = matcher(doc)
for match_id, start, end in matches:
    span = doc[start:end]
    print(f"Matched span: {span.text}")
```

**Output:**

```
Rule-based Matching (Verb + Noun):
Matched span: eats apples
Matched span: drinks water
```

### 8\. Customization and Training

SpaCy allows users to customize pipeline components (e.g., add custom processing steps) and train new models for tasks like NER or Text Classification on your own data.

**Example (illustrating how to add a simple pipeline component):**

```python
import spacy

nlp = spacy.load("en_core_web_sm")

# Define a custom component
def custom_component(doc):
    print("Custom component ran!")
    for token in doc:
        token._.set("is_vowel_start", token.text[0].lower() in "aeiou")
    return doc

# Add a custom attribute to the Token object
from spacy.tokens import Token
Token.set_extension("is_vowel_start", default=False)

# Add the component to the pipeline
nlp.add_pipe("custom_component", last=True)

# Process text
text = "Apple and orange are fruits."
doc = nlp(text)

print("\nCustom Component and Extension:")
for token in doc:
    print(f"{token.text:<10} Is Vowel Start: {token._.is_vowel_start}")
```

**Output:**

```
Custom component ran!

Custom Component and Extension:
Apple      Is Vowel Start: True
and        Is Vowel Start: True
orange     Is Vowel Start: True
are        Is Vowel Start: True
fruits     Is Vowel Start: False
.          Is Vowel Start: False
```

These features make SpaCy a powerful and flexible tool for a wide range of NLP applications, from basic text analysis to building more complex systems.